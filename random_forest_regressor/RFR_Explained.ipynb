{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Explained\n",
    "### Jeanine Buyck\n",
    "\n",
    "This is the code that was used for illustration purposes in the `TODO insert title and link` article.\n",
    "Note that even after a few refactoring sessions, there is still quite a bit of duplicated code.\n",
    "In practice, this notebook would likely get refactored further.\n",
    "However, since the aim here is to clearly demonstrate what is happening at each step,\n",
    "I'm choosing to leave some redundancy in hopes that it will make for easier reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data for use in examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import pi, sqrt, ceil\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules\n",
    "\n",
    "# this adds the parent dir to the path so we can import its modules\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "# this is how we'll generate data for our example\n",
    "from synthetic_data.generate import sine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some controls for plots throughout the notebook\n",
    "default_data_color = 'gray'\n",
    "default_data_alpha = .5\n",
    "default_data_size = 50\n",
    "default_line_color = 'navy'\n",
    "default_dpi = 300\n",
    "print_title = False\n",
    "save_image = True\n",
    "\n",
    "# define some functions for recurring plots and model calls\n",
    "def plot_training_data(X, y):\n",
    "    \"\"\"Plot the training data in a scatter.\"\"\"\n",
    "    plt.scatter(\n",
    "        X,\n",
    "        y,\n",
    "        alpha=default_data_alpha,\n",
    "        c=default_data_color,\n",
    "        s=default_data_size\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_fit(X, y, model):\n",
    "    \"\"\"Gets predictions from model given input X.\"\"\"\n",
    "    # transform shape of X to be what sklearn expects\n",
    "    X_prepped = X.reshape(-1, 1)\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X_prepped, y)\n",
    "\n",
    "    \n",
    "def tree_predict(X, model):\n",
    "    \"\"\"Gets predictions from model given input X.\"\"\"\n",
    "    # transform shape of X to be what sklearn expects\n",
    "    X_prepped = X.reshape(-1, 1)\n",
    "    \n",
    "    # get predictions\n",
    "    y_hat = model.predict(X_prepped)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def plot_model_results(X, y_hat,  c=default_line_color, label=None, legend_line_size=0):\n",
    "    \"\"\"Plots the results of a model as a line.\"\"\"\n",
    "    plt.plot(X, y_hat, c=c, label=label)\n",
    "    plt.legend(\n",
    "        handlelength=legend_line_size,  # controls length of colored line in legend\n",
    "        frameon=False,  # toggles frame around legend on or off\n",
    "        fontsize=12\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set range of x-values for our demonstration\n",
    "min_x = 0\n",
    "max_x = 2 * pi\n",
    "\n",
    "# generate synthetic data based on sine function\n",
    "X, y = sine_data(\n",
    "    min_x=min_x,\n",
    "    max_x=max_x,\n",
    "    n=100,\n",
    "    amplitude=1,\n",
    "    error_mean=0,\n",
    "    error_std_dev=0.1\n",
    ")\n",
    "\n",
    "# plot the data we just created\n",
    "plot_training_data(X, y)\n",
    "if print_title:\n",
    "    plt.title('The Training Data')\n",
    "if save_image:\n",
    "    plt.savefig('training_data.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some x-values for plotting models later on\n",
    "X_model_plot = np.linspace(min_x, max_x, num=10_000, endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a few decision trees of various depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this controls how many trees we'll build\n",
    "tree_count = 9\n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# get number of rows and cols for subplot to plot in a square grid\n",
    "subplot_dim = ceil(sqrt(tree_count))\n",
    "\n",
    "# we're going to build a total of num_trees, each at increasing depth\n",
    "for depth in range(tree_count):\n",
    "    # grow a tree and get its depth\n",
    "    max_depth = depth + 1  # off by one due to range behavior\n",
    "    single_tree = DecisionTreeRegressor(random_state=0, max_depth=max_depth)\n",
    "    tree_fit(X, y, single_tree)\n",
    "    tree_depth = single_tree.get_depth()\n",
    "    \n",
    "    # create our subplot and plot our training data\n",
    "    plt.subplot(subplot_dim, subplot_dim, max_depth)\n",
    "    plot_training_data(X, y)\n",
    "\n",
    "    # generate the predictions for plotting the model\n",
    "    y_model_plot = tree_predict(X_model_plot, single_tree)\n",
    "    \n",
    "    # plot each tree's predictions\n",
    "    plot_model_results(\n",
    "        X=X_model_plot,\n",
    "        y_hat=y_model_plot,\n",
    "        label=f'depth = {tree_depth}'\n",
    "    )\n",
    "    \n",
    "    # this keeps the plot margins tight, without messing up the suptitle! see top answer at:\n",
    "    # https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if print_title:\n",
    "    plt.suptitle('Individual Decision Trees at Various Depths', fontsize=18)\n",
    "if save_image:\n",
    "    plt.savefig(f'{tree_count}_single_trees_diff_depths.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions and constants to help us with our ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that in practice, these functions should live in a module outside of this notebook.\n",
    "# For ease of demonstration, they are being defined here so the reader doesn't need to\n",
    "# leave the notebook while following through the expamples.\n",
    "\n",
    "def ensemble_grow(X, y, num_trees, max_depth, bootstrap=False):\n",
    "    \"\"\"\n",
    "    Grows num_trees individual decision trees, each with max depth of max_depth,\n",
    "    and returns a list of the trained trees.\n",
    "    \n",
    "    Default behavior is to use the full data set (X and y) to grow each tree.\n",
    "    That is, each tree is grown on the same set of data, X and y.\n",
    "    \n",
    "    If bootstrap is set to `True`, then a bootstrapped sample of X and y is used\n",
    "    to train each tree. That is, each tree is grown on its own bootstrapped sample\n",
    "    of the data, where the sample is that same size as the original X and y.\n",
    "    \"\"\"\n",
    "    \n",
    "    # this list will hold the trees we train\n",
    "    trees = []\n",
    "    \n",
    "    for tree in range(num_trees):\n",
    "        if bootstrap:\n",
    "            # default behavior of `resample` is to return a sample with same size as input\n",
    "            train_X, train_y = resample(X, y, replace=True)\n",
    "        else:\n",
    "            # just use the input X and y to train the tree\n",
    "            train_X, train_y = X, y\n",
    "        \n",
    "        ensemble_tree = DecisionTreeRegressor(random_state=0, max_depth=max_depth)\n",
    "        tree_fit(train_X, train_y, ensemble_tree)\n",
    "        \n",
    "        trees.append(ensemble_tree)\n",
    "        \n",
    "    return trees\n",
    "\n",
    "\n",
    "def ensemble_predict(X, trees):\n",
    "    \"\"\"Get predictions for X by averaging predictions from the trees.\"\"\"\n",
    "    \n",
    "    # keep track of number of trees for computing the average later\n",
    "    num_trees = len(trees)\n",
    "    \n",
    "    # start with zeros, we will add predictions to this for each tree\n",
    "    y_hat_sum = np.zeros(len(X))\n",
    "    \n",
    "    # get the predictions for each tree and add to running sum\n",
    "    for tree in trees:\n",
    "        y_hat_sum += tree_predict(X, tree)\n",
    "        \n",
    "    # divide by total number of trees to get the average\n",
    "    y_hat = y_hat_sum / num_trees\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "# constants\n",
    "num_trees=6\n",
    "max_depth=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an ensemble of trees with the same max_depth, no randomness added in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ensemble of trees with no randomness added in\n",
    "ensemble_deterministic = ensemble_grow(\n",
    "    X=X, \n",
    "    y=y, \n",
    "    num_trees=num_trees, \n",
    "    max_depth=max_depth,\n",
    "    bootstrap=False\n",
    ")    \n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# get number of rows and cols for subplot to plot in a square grid\n",
    "subplot_dim = ceil(sqrt(num_trees))\n",
    "\n",
    "# plot the model for each tree in the ensemble on its own graph\n",
    "for idx, tree in enumerate(ensemble_deterministic):\n",
    "    tree_depth = tree.get_depth()\n",
    "    \n",
    "    # create our subplot and plot our training data\n",
    "    plt.subplot(subplot_dim, subplot_dim, idx+1)\n",
    "    plot_training_data(X, y)\n",
    "    \n",
    "    # generate the predictions for plotting the model\n",
    "    y_model_plot = tree_predict(X_model_plot, tree)\n",
    "\n",
    "    # plot each tree's predictions\n",
    "    plot_model_results(\n",
    "        X=X_model_plot,\n",
    "        y_hat=y_model_plot,\n",
    "        label=f'tree {idx + 1}'\n",
    "    )\n",
    "    \n",
    "    # this keeps the plot margins tight, without messing up the suptitle! see top answer at:\n",
    "    # https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if print_title:\n",
    "    plt.suptitle('Individual Trees in Ensemble', fontsize=18)\n",
    "if save_image:\n",
    "    plt.savefig('trees_in_determinisic_ensemble.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot side by side: all decision tree models on one graph, next to the ensemble on another\n",
    "# GOAL: show without bootstrapping we have not changed the model results\n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot the model for each tree within the ensemble on the same graph\n",
    "plt.subplot(1, 2, 1)\n",
    "if print_title:\n",
    "    plt.title(f'Individual Decision Trees', fontsize=14)\n",
    "    \n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "for idx, tree in enumerate(ensemble_deterministic):\n",
    "    tree_depth = tree.get_depth()\n",
    "    \n",
    "    # generate the predictions for plotting the model\n",
    "    y_model_plot = tree_predict(X_model_plot, tree)\n",
    "    \n",
    "    # plot each tree's predictions\n",
    "    plot_model_results(\n",
    "        X=X_model_plot,\n",
    "        y_hat=y_model_plot,\n",
    "        c=None,\n",
    "        label=f'tree {idx + 1}',\n",
    "        legend_line_size=2\n",
    "    )\n",
    "\n",
    "# plot the results of ensemble on another graph\n",
    "plt.subplot(1, 2, 2)\n",
    "if print_title:\n",
    "    plt.title(f'An Ensemble of Trees', fontsize=14)\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = ensemble_predict(X_model_plot, ensemble_deterministic)\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'ensemble of {num_trees} trees',\n",
    "    legend_line_size=2\n",
    ")\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if save_image:\n",
    "    plt.savefig('determinisic_ensemble_side_by_side.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an ensemble of trees with the same max_depth, but bootstrap the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ensemble of trees with bootstrapped training sets\n",
    "ensemble_bootstrapped = ensemble_grow(\n",
    "    X=X, \n",
    "    y=y, \n",
    "    num_trees=num_trees, \n",
    "    max_depth=max_depth,\n",
    "    bootstrap=True\n",
    ")    \n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# get number of rows and cols for subplot to plot in a square grid\n",
    "subplot_dim = ceil(sqrt(num_trees))\n",
    "\n",
    "# plot the model for each tree in the ensemble on its own graph\n",
    "for idx, tree in enumerate(ensemble_bootstrapped):\n",
    "    tree_depth = tree.get_depth()\n",
    "    \n",
    "    # create our subplot and plot our training data\n",
    "    plt.subplot(subplot_dim, subplot_dim, idx+1)\n",
    "    plot_training_data(X, y)\n",
    "    \n",
    "    # generate the predictions for plotting the model\n",
    "    y_model_plot = tree_predict(X_model_plot, tree)\n",
    "    \n",
    "    # plot each tree's predictions\n",
    "    plot_model_results(\n",
    "        X=X_model_plot,\n",
    "        y_hat=y_model_plot,\n",
    "        label=f'tree {idx + 1}'\n",
    "    )\n",
    "    \n",
    "    # this keeps the plot margins tight, without messing up the suptitle! see top answer at:\n",
    "    # https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if print_title: \n",
    "    plt.suptitle('Individual Trees in Ensemble with Bootstrapping', fontsize=18)\n",
    "if save_image:\n",
    "    plt.savefig('trees_in_bootstrapped_ensemble.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot side by side: all decision tree models on one graph, next to the ensemble on another\n",
    "# GOAL: see that not only does the ensemble produce a different result,\n",
    "#       but also the curve is becoming smoother and not overfitting the data.\n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot the model for each tree within the ensemble on the same graph\n",
    "plt.subplot(1, 2, 1)\n",
    "if print_title:\n",
    "    plt.title(f'Individual Decision Trees with Bootstrapping', fontsize=14)\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "for idx, tree in enumerate(ensemble_bootstrapped):\n",
    "    tree_depth = tree.get_depth()\n",
    "    \n",
    "    # generate the predictions for plotting the model\n",
    "    y_model_plot = tree_predict(X_model_plot, tree)\n",
    "    \n",
    "    # plot each tree's predictions\n",
    "    plot_model_results(\n",
    "        X=X_model_plot,\n",
    "        y_hat=y_model_plot,\n",
    "        c=None,\n",
    "        label=f'tree {idx + 1}',\n",
    "        legend_line_size=2\n",
    "    )\n",
    "\n",
    "# plot the results of ensemble on another graph\n",
    "plt.subplot(1, 2, 2)\n",
    "if print_title:\n",
    "    plt.title(f'An Ensemble of Trees with Bootstrapping', fontsize=14)\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = ensemble_predict(X_model_plot, ensemble_bootstrapped)\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'ensemble of {num_trees} trees',\n",
    "    legend_line_size=2\n",
    ")\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if save_image:\n",
    "    plt.savefig('bootstrapped_ensemble_side_by_side.png', dpi=default_dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that a random forest _can_ overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with random forest from above and do two variations\n",
    "# 1. let number of decision trees increase dramatically, but keep max_depth fixed and small\n",
    "# 2. keep number of decision trees fixed, but let max_depth get big\n",
    "\n",
    "# GOAL: show that increasing the number of trees does not overfit the data but does smooth the curve\n",
    "#       AND that averaging is not enough to correct for overfitting if the individual trees are too deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a factor for making the number of trees or max depth larger\n",
    "scaling_factor = 3\n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Part 0: plot our previous ensemble for easy reference\n",
    "plt.subplot(1, 3, 1)\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f'Random Forest {num_trees} Trees with Depth {max_depth}')\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = ensemble_predict(X_model_plot, ensemble_bootstrapped)\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{num_trees} tree ensemble\\n max depth {max_depth}'\n",
    ")\n",
    "\n",
    "# Part 1: let number of decision trees increase dramatically, but keep max_depth fixed and small\n",
    "plt.subplot(1, 3, 2)\n",
    "    \n",
    "more_trees = scaling_factor * num_trees\n",
    "many_tree_ensemble = ensemble_grow(\n",
    "    X=X, \n",
    "    y=y, \n",
    "    num_trees=more_trees,  # increase from previous tree count \n",
    "    max_depth=max_depth,   # leave max_depth fixed as in original\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f'Random Forest of {more_trees} Trees with Depth {max_depth}')\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = ensemble_predict(X_model_plot, many_tree_ensemble)\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{more_trees} tree ensemble\\n max depth {max_depth}'\n",
    ")\n",
    "\n",
    "# Part 2: keep number of decision trees fixed, but let max_depth get big\n",
    "plt.subplot(1, 3, 3)\n",
    "    \n",
    "more_depth = scaling_factor * max_depth\n",
    "deep_tree_ensemble = ensemble_grow(\n",
    "    X=X, \n",
    "    y=y, \n",
    "    num_trees=num_trees,   # leave tree count fixed as in original\n",
    "    max_depth=more_depth,  # increase from previous max_depth\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f'Random Forest of {num_trees} Trees with Depth {more_depth}')\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = ensemble_predict(X_model_plot, deep_tree_ensemble)\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{num_trees} tree ensemble\\n max depth {more_depth}'\n",
    ")\n",
    "\n",
    "# this keeps the plot margins tight, without messing up the suptitle! see top answer at:\n",
    "# https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if save_image:\n",
    "    plt.savefig('random_forest_num_trees_vs_depth.png', dpi=default_dpi, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our results to those from sci-kit learn's `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similar results to the three above, but use sci-kit learn's RFR instead of our ensemble\n",
    "\n",
    "# figure size to keep all of the subplots legible\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Part 0: plot scikit-learn's RFR with our original number of trees and original max depth\n",
    "plt.subplot(1, 3, 1)\n",
    "    \n",
    "base_sklearn_rfr = RandomForestRegressor(\n",
    "    n_estimators=num_trees,\n",
    "    max_depth=max_depth,\n",
    "    bootstrap=True,\n",
    "    max_samples=None,  # so bootstrap sample will have same size as original data\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "base_sklearn_rfr.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f\"Scikit-learn's RFR of {num_trees} Trees with Depth {max_depth}\")\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = base_sklearn_rfr.predict(X_model_plot.reshape(-1, 1))\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{num_trees} tree RFR\\n max depth {max_depth}'\n",
    ")\n",
    "\n",
    "    \n",
    "# Part 1: let number of decision trees increase dramatically, but keep max_depth fixed and small\n",
    "plt.subplot(1, 3, 2)\n",
    "    \n",
    "base_sklearn_rfr = RandomForestRegressor(\n",
    "    n_estimators=more_trees,  # increase from previous tree count \n",
    "    max_depth=max_depth,   # leave max_depth fixed as in original\n",
    "    bootstrap=True,\n",
    "    max_samples=None,  # so bootstrap sample will have same size as original data\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "base_sklearn_rfr.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f\"Scikit-learn's RFR of {more_trees} Trees with Depth {max_depth}\")\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = base_sklearn_rfr.predict(X_model_plot.reshape(-1, 1))\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{more_trees} tree RFR\\n max depth {max_depth}'\n",
    ")\n",
    "\n",
    "# Part 2: keep number of decision trees fixed, but let max_depth get big\n",
    "plt.subplot(1, 3, 3)\n",
    "    \n",
    "base_sklearn_rfr = RandomForestRegressor(\n",
    "    n_estimators=num_trees,   # leave tree count fixed as in original\n",
    "    max_depth=more_depth,  # increase from previous max_depth\n",
    "    bootstrap=True,\n",
    "    max_samples=None,  # so bootstrap sample will have same size as original data\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "base_sklearn_rfr.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "if print_title:\n",
    "    plt.title(f\"Scikit-learn's RFR of {num_trees} Trees with Depth {more_depth}\")\n",
    "\n",
    "# plot our training data\n",
    "plot_training_data(X, y)\n",
    "\n",
    "# generate the predictions for plotting the model\n",
    "y_model_plot = base_sklearn_rfr.predict(X_model_plot.reshape(-1, 1))\n",
    "\n",
    "# plot the ensemble's predictions\n",
    "plot_model_results(\n",
    "    X=X_model_plot,\n",
    "    y_hat=y_model_plot,\n",
    "    label=f'{num_trees} tree RFR\\n max depth {more_depth}'\n",
    ")\n",
    "\n",
    "# this keeps the plot margins tight, without messing up the suptitle! see top answer at:\n",
    "# https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# final touches on our plot and saving to file\n",
    "if save_image:\n",
    "    plt.savefig('sklearn_rfr_num_trees_vs_depth.png', dpi=default_dpi, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
